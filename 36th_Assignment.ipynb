{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "36th Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIzgtnXR/XutQafZKvjw56",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pafernannapi18/GithubIntro/blob/main/36th_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPx0jxgYg0ur"
      },
      "outputs": [],
      "source": [
        "#Download IMDB to the current folder\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# Unzip\n",
        "!tar zxf aclImdb_v1.tar.gz\n",
        "# aclImdb / train / unsup is unlabeled and removed\n",
        "!rm -rf aclImdb/train/unsup\n",
        "# Show IMDB dataset description\n",
        "!cat aclImdb/README"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the files\n",
        "from sklearn.datasets import load_files\n",
        "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
        "x_train, y_train = train_review.data, train_review.target\n",
        "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
        "x_test, y_test = test_review.data, test_review.target\n",
        "# Display of the correspondence between 0, 1 of the label and the meaning\n",
        "print(train_review.target_names)"
      ],
      "metadata": {
        "id": "03uCkakbhPzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x : {}\".format(x_train[0]))"
      ],
      "metadata": {
        "id": "SNFBxmwehSKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mini_dataset = \\\n",
        "  [\"This movie is very good.\",\n",
        "  \"This film is a good\",\n",
        "  \"Very bad. Very, very bad.\"]"
      ],
      "metadata": {
        "id": "XXFuxRobhUtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
        "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "# Put together in DataFrame\n",
        "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "metadata": {
        "id": "bg4nKxDahXCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the range of n-gram used in ngram_range\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "metadata": {
        "id": "nh-jEBOWha6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 1] Scratch implementation of BoW"
      ],
      "metadata": {
        "id": "PVfIzg7uhh9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_line = 'This movie is SOOOO funny!!!'.lower().replace('!', '').split()\n",
        "second_line = 'What a movie! I never'.lower().replace('!', '').split()\n",
        "third_line = 'best movie ever!!!!! this movie'.lower().replace('!', '').split()"
      ],
      "metadata": {
        "id": "VayTaU27hfzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gram_1_feature_names = first_line + second_line + third_line"
      ],
      "metadata": {
        "id": "0NWCWLOQhqTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "gram_1 = pd.DataFrame(np.zeros((3, len(set(gram_1_feature_names)))).astype('int'), columns=set(gram_1_feature_names))\n",
        "\n",
        "for i, ss in enumerate([first_line, second_line, third_line]):\n",
        "    for s in ss:\n",
        "        n = ss.count(s)\n",
        "        gram_1[s][i] = n\n",
        "\n",
        "gram_1\n"
      ],
      "metadata": {
        "id": "h325Y1E8h5T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gram_2_feature_names = []\n",
        "gram_2_lines = []\n",
        "for i, s in enumerate([first_line, second_line, third_line]):\n",
        "  line = []\n",
        "  for ss in range(len(s)-1):\n",
        "    line.append(f'{s[ss]} {s[ss+1]}')\n",
        "    gram_2_feature_names.append(f'{s[ss]} {s[ss+1]}')\n",
        "  gram_2_lines.append(line)"
      ],
      "metadata": {
        "id": "8MVfuIMUh8qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gram_2 = pd.DataFrame(np.zeros((3, len(set(gram_2_feature_names)))).astype('int'), columns=set(gram_2_feature_names))\n",
        "\n",
        "for i, ss in enumerate(gram_2_lines):\n",
        "    for s in ss:\n",
        "        n = ss.count(s)\n",
        "        gram_2[s][i] = n\n",
        "        \n",
        "gram_2"
      ],
      "metadata": {
        "id": "bpg0yNJ_iBjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDF"
      ],
      "metadata": {
        "id": "z1ck46K_iFyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "n_samples = 25000\n",
        "idf = np.log(n_samples/np.arange(1,n_samples))\n",
        "plt.title(\"IDF\")\n",
        "plt.xlabel(\"df(t)\")\n",
        "plt.ylabel(\"IDF\")\n",
        "plt.plot(idf)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F0Ubm11-iGvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "metadata": {
        "id": "04FqJdbqiLrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Stopword for the first time\n",
        "import nltk\n",
        "stop_words = nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
      ],
      "metadata": {
        "id": "OMioQytciOcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "metadata": {
        "id": "EtN1QF0niQ89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problem 2] TF-IDF calculation"
      ],
      "metadata": {
        "id": "KnqL8pDhiTtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words= stop_words, max_features=5000)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "X_test = vectorizer.fit_transform(x_test)"
      ],
      "metadata": {
        "id": "nmT-mCGQiVAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "mURqchtziZXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problem 3] Learning using TF-IDF"
      ],
      "metadata": {
        "id": "sWKA6V6SicbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "lgb = lgb.LGBMClassifier().fit(X_train,y_train)\n",
        "y_pred = lgb.predict(X_test)"
      ],
      "metadata": {
        "id": "XQJTF0piid4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"{}\".format(lgb.score(X_test, y_test)))\n",
        "print(\"{}\".format(precision_score(y_test,y_pred)))\n",
        "print(\"{}\".format(recall_score(y_test,y_pred)))\n",
        "print(\"{}\".format(f1_score(y_test,y_pred)))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "A1ISLPLaimw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
        "model = Word2Vec(min_count=1, size=10) # Set the number of dimensions to 10\n",
        "model.build_vocab(sentences) # Preparation\n",
        "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) #Learning\n",
        "\n",
        "print(\"List of vocabulary : {}\".format(model.wv.vocab.keys()))\n",
        "for vocab in model.wv.vocab.keys():\n",
        "  print(\"Vector of {}: \\n{}\".format(vocab, model.wv[vocab]))"
      ],
      "metadata": {
        "id": "E4L-SwqWiszv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=\"good\", topn=3)"
      ],
      "metadata": {
        "id": "fTh-KniRix8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "vocabs = model.wv.vocab.keys()\n",
        "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
        "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
        "for i, word in enumerate(list(vocabs)):\n",
        "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
        "ax.set_yticklabels([])\n",
        "ax.set_xticklabels([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o2vxLyKdi0Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Distributed representation of IMDB movie review datasets"
      ],
      "metadata": {
        "id": "9uNV-fami7TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = Word2Vec(min_count=1, size=10) # Set the number of dimensions to 10\n",
        "model_2.build_vocab(x_train) # Preparation\n",
        "model_2.train(x_train, total_examples=model_2.corpus_count, epochs=model_2.iter) #Learning"
      ],
      "metadata": {
        "id": "ge8BzUobi-Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problem 5] Corpus pretreatment"
      ],
      "metadata": {
        "id": "E5nOToQ0jELR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with_url = 0\n",
        "for i, s in enumerate(x_train):\n",
        "    if 'www' in s:\n",
        "      with_url = i\n",
        "      print('before processing')\n",
        "      print('-----')\n",
        "      print(s)\n",
        "      break"
      ],
      "metadata": {
        "id": "2uXlGaRBjGD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "before processing"
      ],
      "metadata": {
        "id": "eTyGTVSKjWw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "no_preprocessing = x_train[with_url]\n",
        "after_preprocessing1 = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-â€¦]+', \"\", no_preprocessing) \n",
        "after_preprocessing2 = re.sub(r'<[^>]+>', \" \", after_preprocessing1) \n",
        "after_preprocessing3 = re.sub(r\"[^0-9a-zA-Z ]\", \"\", after_preprocessing2) \n",
        "after_preprocessing = after_preprocessing3.lower() \n",
        "print('after processing')\n",
        "print('-----')\n",
        "print(after_preprocessing)"
      ],
      "metadata": {
        "id": "KiFkL6jHjb9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "after processing"
      ],
      "metadata": {
        "id": "4SwoPIUhjWpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problem 6] Learning Word2Vec"
      ],
      "metadata": {
        "id": "YxoqN2O7joDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(min_count=1, size=10) \n",
        "model.build_vocab(after_preprocessing) \n",
        "model.train(after_preprocessing, total_examples=model.corpus_count, epochs=model.iter)"
      ],
      "metadata": {
        "id": "HcbnSwZ5jgXj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}